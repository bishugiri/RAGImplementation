{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "360ff05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fcbbf856",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmasterdf = pd.DataFrame(columns = [\"all_text\", \"url\"])\\n\\nfor i in range(1, 10320):\\n    \\n    try:\\n        # Specify the URL you want to scrape\\n        url = \\'https://nkp.gov.np/full_detail/\\'+str(i)\\n\\n        # Send an HTTP GET request to the URL\\n        response = requests.get(url, verify = False)\\n\\n        # encoding\\n        response.encoding = \\'utf-8\\'\\n\\n        # Get the HTML content from the response\\n        html_content = response.text\\n\\n        # Parse the HTML content using BeautifulSoup\\n        soup = BeautifulSoup(html_content, \\'html.parser\\')\\n\\n        # Optionally, print the formatted HTML\\n        faisala_div = soup.find(\"div\", id=\"faisala_detail \")\\n        #print(faisala_div)\\n        df = pd.DataFrame({\"all_text\": [str(faisala_div)], \"url\": [url]})\\n        masterdf = pd.concat([masterdf, df])\\n    \\n    except:\\n        pass\\n\\n'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "masterdf = pd.DataFrame(columns = [\"all_text\", \"url\"])\n",
    "\n",
    "for i in range(1, 10320):\n",
    "    \n",
    "    try:\n",
    "        # Specify the URL you want to scrape\n",
    "        url = 'https://nkp.gov.np/full_detail/'+str(i)\n",
    "\n",
    "        # Send an HTTP GET request to the URL\n",
    "        response = requests.get(url, verify = False)\n",
    "\n",
    "        # encoding\n",
    "        response.encoding = 'utf-8'\n",
    "\n",
    "        # Get the HTML content from the response\n",
    "        html_content = response.text\n",
    "\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Optionally, print the formatted HTML\n",
    "        faisala_div = soup.find(\"div\", id=\"faisala_detail \")\n",
    "        #print(faisala_div)\n",
    "        df = pd.DataFrame({\"all_text\": [str(faisala_div)], \"url\": [url]})\n",
    "        masterdf = pd.concat([masterdf, df])\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "78c00bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Parquet file into a DataFrame\n",
    "df = pd.read_parquet(\"C:\\\\Users\\\\Giri\\\\Desktop\\\\Sankhya\\\\Legal\\\\legaldata.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4811f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract only Devanagari text\n",
    "def extract_devanagari(html_text):\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")  # Remove HTML tags\n",
    "    text = soup.get_text()\n",
    "    devnagari_text = re.findall(r'[\\u0900-\\u097F\\s।,]+', text)  # Keep only Devanagari characters\n",
    "    return ''.join(devnagari_text).strip()  # Join and remove leading/trailing spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ac1730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_judges(text):\n",
    "    if pd.isna(text):  # Handle missing values\n",
    "        return None\n",
    "    \n",
    "    pattern = r'माननीय न्यायाधीश [श्री]*\\s*([\\u0900-\\u097F\\s]+)'\n",
    "    judges = re.findall(pattern, text)\n",
    "    \n",
    "    return [name.strip() for name in judges] if judges else None  # Return cleaned list or None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b3650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the first Nepali date (YYYY।MM।DD)\n",
    "def extract_nepali_date(text):\n",
    "    if pd.isna(text):  # Handle missing values\n",
    "        return None\n",
    "    \n",
    "    pattern = r'\\d{4}।\\d{1,2}।\\d{1,2}'  # Matches YYYY।MM।DD\n",
    "    match = re.search(pattern, text)  # Find first match\n",
    "    \n",
    "    return match.group() if match else None  # Return date or None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0c40ed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to the DataFrame column\n",
    "df['clean_text'] = df['all_text'].apply(extract_devanagari)\n",
    "df['judges'] = df['clean_text'].apply(extract_judges)\n",
    "df['decision_date'] = df['clean_text'].apply(extract_nepali_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83cb008",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
